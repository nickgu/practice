# configuration for cifar10 network.

# try :
#   baseline + (residual + 7-layers)
[cifar10_cnn7_residual]
input_def=f:4
label_def=i:1
layers=c0,c1,c1_2,c2,reshape,relu,softmax_in,out,cost
active=out
cost=cost

learner=adam
learning_rate_type=exponential_decay
learning_rate=1e-3
learning_decay_ratio=0.98
learning_decay_step=1000

epoch=500
batch_size=128

# c0 feature extractor.
c0.type=conv2d_pool
c0.input=input[0]
c0.shape=3,3,3,192
c0.pool_type=avg
c0.pool_size=2
c0.pool_strides=2

# c1 has 5-layers (5*128)
c1.type=conv2d_pool
c1.input=c0
c1.shape=3,3,192,128
c1.use_residual=1
c1.stack_count=2

c1_2.type=conv2d_pool
c1_2.input=c1
c1_2.shape=3,3,128,128
c1_2.use_residual=1
c1_2.stack_count=3
c1_2.pool_type=avg
c1_2.pool_size=2
c1_2.pool_strides=2

c2.type=conv2d_pool
c2.input=c1_2
c2.shape=3,3,128,256
c2.pool_type=avg
c2.pool_size=2
c2.pool_strides=2

reshape.type=reshape
reshape.input=c2
reshape.shape=-1,2304

relu.type=full_connect_op
relu.input=reshape
relu.op=relu
relu.l2wd=0.004
relu.n_in=2304
relu.n_out=1024
relu.weight_stddev=0.04
relu.bias_init=0.05

softmax_in.type=full_connect
softmax_in.input=relu
softmax_in.n_in=1024
softmax_in.n_out=10
softmax_in.l2wd=0.00
softmax_in.weight_stddev=0.0052

out.type=softmax
out.input=softmax_in

cost.type=softmax_entropy
cost.input=softmax_in,__label__




# try :
#   baseline + (residual + 5-layers)
#  88.2% finally
[cifar10_cnn5_residual]
input_def=f:4
label_def=i:1
layers=c0,c1,c2,reshape,relu,softmax_in,out,cost
active=out
cost=cost

learner=adam
learning_rate_type=exponential_decay
learning_rate=1e-3
learning_decay_ratio=0.98
learning_decay_step=1000

epoch=500
batch_size=128

# c0 feature extractor.
c0.type=conv2d_pool
c0.input=input[0]
c0.shape=3,3,3,192
c0.pool_type=avg
c0.pool_size=2
c0.pool_strides=2

# c1 has 3-layers
c1.type=conv2d_pool
c1.input=c0
c1.shape=3,3,192,256
c1.use_residual=1
c1.stack_count=3
c1.pool_type=avg
c1.pool_size=2
c1.pool_strides=2

c2.type=conv2d_pool
c2.input=c1
c2.shape=3,3,256,256
c2.pool_type=avg
c2.pool_size=2
c2.pool_strides=2

reshape.type=reshape
reshape.input=c2
reshape.shape=-1,2304

relu.type=full_connect_op
relu.input=reshape
relu.op=relu
relu.l2wd=0.004
relu.n_in=2304
relu.n_out=1024
relu.weight_stddev=0.04
relu.bias_init=0.05

softmax_in.type=full_connect
softmax_in.input=relu
softmax_in.n_in=1024
softmax_in.n_out=10
softmax_in.l2wd=0.00
softmax_in.weight_stddev=0.0052

out.type=softmax
out.input=softmax_in

cost.type=softmax_entropy
cost.input=softmax_in,__label__



# test baseline.
# fetch 87% ! @53600 iteration.(137 epoch)
# fetch 87.8% ! finally
[cifar10_cnn3_fc]
input_def=f:4
label_def=i:1
layers=c0,c1,c2,reshape,relu,softmax_in,out,cost
active=out
cost=cost

learner=adam
learning_rate_type=exponential_decay
learning_rate=1e-3
learning_decay_ratio=0.98
learning_decay_step=1000

epoch=500
batch_size=128

c0.type=conv2d_pool
c0.input=input[0]
c0.shape=3,3,3,192
c0.pool_type=avg
c0.pool_size=2
c0.pool_strides=2

c1.type=conv2d_pool
c1.input=c0
c1.shape=3,3,192,256
c1.pool_type=avg
c1.pool_size=2
c1.pool_strides=2

c2.type=conv2d_pool
c2.input=c1
c2.shape=3,3,256,256
c2.pool_type=avg
c2.pool_size=2
c2.pool_strides=2

reshape.type=reshape
reshape.input=c2
reshape.shape=-1,2304

relu.type=full_connect_op
relu.input=reshape
relu.op=relu
relu.l2wd=0.004
relu.n_in=2304
relu.n_out=1024
relu.weight_stddev=0.04
relu.bias_init=0.05

softmax_in.type=full_connect
softmax_in.input=relu
softmax_in.n_in=1024
softmax_in.n_out=10
softmax_in.l2wd=0.00
softmax_in.weight_stddev=0.0052

out.type=softmax
out.input=softmax_in

cost.type=softmax_entropy
cost.input=softmax_in,__label__

# this is a simple multi-layer3 net.
[cifar10_simple_layer3]
input_def=f:4
label_def=i:1
layers=cost,cp1,cp2,cp3,reshape,relu,relu2,softmax_in,out
active=out
cost=cost

learner=adam
learning_rate_type=exponential_decay
learning_rate=1e-3
learning_decay_ratio=0.96
learning_decay_step=1000

epoch=500
batch_size=128

cp1.type=conv2d_pool
cp1.input=input[0]
cp1.shape=5,5,3,192
cp1.pool_type=avg
cp1.pool_size=3
cp1.pool_strides=2

cp2.type=conv2d_pool
cp2.input=cp1
cp2.shape=5,5,192,64
cp2.pool_type=max
cp2.pool_size=3
cp2.pool_strides=2

cp3.type=conv2d_pool
cp3.input=cp2
cp3.shape=5,5,64,64
cp3.pool_type=max
cp3.pool_size=3
cp3.pool_strides=2

reshape.type=reshape
reshape.input=cp3
reshape.shape=-1,576

relu.type=full_connect_op
relu.input=reshape
relu.op=relu
relu.l2wd=0.004
relu.n_in=576
relu.n_out=256
relu.weight_stddev=0.04
relu.bias_init=0.1

relu2.type=full_connect_op
relu2.input=relu
relu2.op=relu
relu2.l2wd=0.004
relu2.n_in=256
relu2.n_out=192
relu2.weight_stddev=0.04
relu2.bias_init=0.1

softmax_in.type=full_connect
softmax_in.input=relu2
softmax_in.n_in=192
softmax_in.n_out=10
softmax_in.l2wd=0.00
softmax_in.weight_stddev=0.0052

out.type=softmax
out.input=softmax_in

cost.type=softmax_entropy
cost.input=softmax_in,__label__


# this net is copied from tensorflow tutorial.
# This network can fetch ~ 85% 86% precision.
[cifar10_tf_tutor]
input_def=f:4
label_def=i:1
layers=cost,convpool1,norm1,conv2,norm2,pool2,reshape2,relu,relu2,softmax_in,out
active=out
cost=cost

learner=adam
learning_rate=1e-3

#learner=movingGradient
#learning_rate_type=exponential_decay
#learning_rate=1e-1
#learning_decay_ratio=0.1
#learning_decay_step=130000

epoch=500
batch_size=128

convpool1.type=conv2d_pool
convpool1.input=input[0]
convpool1.shape=5,5,3,64
convpool1.pool_type=max
convpool1.pool_size=3
convpool1.pool_strides=2

norm1.type=local_norm
norm1.input=convpool1

conv2.type=conv2d
conv2.input=norm1
conv2.shape=3,3,64,64

norm2.type=local_norm
norm2.input=conv2

pool2.type=pooling
pool2.input=norm2
pool2.pool_type=max
pool2.pool_size=3
pool2.pool_strides=2

reshape2.type=reshape
reshape2.input=pool2
# 2304@24 4096@32
reshape2.shape=-1,2304

relu.type=full_connect_op
relu.input=reshape2
relu.op=relu
relu.l2wd=0.004
# 2304@24 4096@32
relu.n_in=2304
relu.n_out=384
relu.weight_stddev=0.04
relu.bias_init=0.1

relu2.type=full_connect_op
relu2.input=relu
relu2.op=relu
relu2.l2wd=0.004
relu2.n_in=384
relu2.n_out=192
relu2.weight_stddev=0.04
relu2.bias_init=0.1

softmax_in.type=full_connect
softmax_in.input=relu2
softmax_in.n_in=192
softmax_in.n_out=10
softmax_in.l2wd=0.00
softmax_in.weight_stddev=0.0052

out.type=softmax
out.input=softmax_in

cost.type=softmax_entropy
cost.input=softmax_in,__label__

